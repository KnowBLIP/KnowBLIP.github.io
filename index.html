<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KnowBLIP: Large Vision-Language Models meet Interleaved World Knowledge</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        .container {
            width: 80%;
            margin: 0 auto;
            padding: 20px;
        }
        h1 {
            text-align: center;
            margin-bottom: 20px;
        }
        .links {
            text-align: center;
            margin-bottom: 20px;
        }
        .links a {
            margin: 0 10px;
            text-decoration: none;
            color: #007BFF;
        }
        .section {
            margin-bottom: 40px;
        }
        .section img {
            width: 100%;
            height: auto;
            margin-bottom: 20px;
        }
        .framework-images {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        .framework-images img {
            width: 100%;
            height: auto;
        }
        hr {
            border: 0;
            height: 1px;
            background: #333;
            background-image: linear-gradient(to right, #ccc, #333, #ccc);
            margin-bottom: 40px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        table, th, td {
            border: 1px solid black;
        }
        th, td {
            padding: 8px;
            text-align: center;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>KnowBLIP: Large Vision-Language Models meet Interleaved World Knowledge</h1>
        <div class="links">
            <a href="https://yourcloudservice.com/yourpaper.pdf">Paper</a> | <a href="https://github.com/yourusername/yourrepository">Code and Dataset</a>
        </div>
        <div class="section">
            <h2>Abstract</h2>
            <p>Currently, Large Visual Language Models (LVLMs) have integrated various tasks, and some \emph{``any2any''} models have emerged, enabling support for diverse modalities for input and various output formats, thereby expanding the impact of LVLMs. However, we discover a common issue —the generated content do not align with the objective facts of the real world — we refer to as \emph{``A gap with world knowledge''}. Furthermore, the poor performance of text generation ($\leq$25\% ) and image generation ($\leq$5\% ) tasks around world knowledge validate that existing ``any2any'' models rely on imagining rather than understanding world to generate content. In this paper, we develop a comprehensive multi-modal benchmark to foster a close connection between ``any2any'' LVLMs and the real world. Specifically, we have collected a dataset, IWK-300k, of multi-modal objective knowledge derived from reasoning on multi-modal knowledge graphs, characterized by interleaved, multi-turn, and multi-lingual attributes. Furthermore, we have developed a KnowBLIP, capable of accommodating various input and output paradigms based on IWK-300k dataset. Our KnowBLIP outperformed 8 leading baselines, such as GPT-4V and Qwen-VL, setting a new record for state-of-the-art performance in downstream multi-modal tasks.</p>
        </div>
        <hr>
        <div class="section">
            <h2>Framework</h2>
            <p>To bridge the gap between LVLMs and the visible real world, we propose a novel multi-modal framework, KnowBLIP, designed to support a comprehensive, world-grounded, ``any2any'' paradigm.</p>
            <img src="./framework.png" alt="Dataset images">
        </div>
        <hr>
        <div class="section">
            <h2>Case Study</h2>
            <p>The four formats of the IWK-300K dataset endow KnowBLIP with a diverse array of functionalities, enabling it to handle a wide range of tasks and scenarios related to real world knowledge. To illustrate these capabilities, we present several concrete user cases.</p>
            <div class="framework-images">
                <img src="./kuancase11.jpg" alt="Framework image 1">
                <img src="./kuancase22.jpg" alt="Framework image 2">
                <img src="./kuancase33.jpg" alt="Framework image 3">
                <img src="./kuancase44.jpg" alt="Framework image 1">
                <img src="./kuancase55.jpg" alt="Framework image 2">
                <img src="./kuancase66.jpg" alt="Framework image 3">
            </div>
        </div>
        <hr>
        <div class="section">
            <h2>Leaderboard</h2>
            <p>We evaluate various models including LLMs and LMMs. In each type, we consider both closed- and open-source models. Our evaluation is conducted under a zero-shot setting to assess the capability of models to generate accurate answers without fine-tuning or few-shot demonstrations on our benchmark. For all models, we use the default prompt provided by each model for multi-choice or open QA, if available. If models do not provide prompts for task types in MMU, we conduct prompt engineering on the validation set and use the most effective prompt for the later zero-shot experiment.</p>
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Text2Text Acc</th>
                        <th>Image2Text Acc</th>
                        <th>Text2Image R@1</th>
                        <th>Text2Image R@5</th>
                        <th>Image2Image R@1</th>
                        <th>Image2Image R@5</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>VisualGLM</td>
                        <td>8.54%</td>
                        <td>7.53%</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>Qwen-VL</td>
                        <td>14.57%</td>
                        <td>12.56%</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>MiniGPTV2</td>
                        <td>12.06%</td>
                        <td>1.50%</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>GILL</td>
                        <td>8.10%</td>
                        <td>11.75%</td>
                        <td>0.00%</td>
                        <td>2.95%</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>TextBind</td>
                        <td>18.20%</td>
                        <td>21.02%</td>
                        <td>2.98%</td>
                        <td>4.13%</td>
                        <td>2.21%</td>
                        <td>4.70%</td>
                    </tr>
                    <tr>
                        <td>LLaVA-1.5-7B</td>
                        <td>6.09%</td>
                        <td>10.20%</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>CogVLM-chat-v1.1</td>
                        <td>14.45%</td>
                        <td>17.41%</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>GPT-4V</td>
                        <td>61.53%</td>
                        <td>55.88%</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>Gemini-1.5-pro</td>
                        <td>56.41%</td>
                        <td>41.53%</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</body>
</html>
