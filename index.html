<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KnowBLIP: Large Vision-Language Models meet Interleaved World Knowledge</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        .container {
            width: 80%;
            margin: 0 auto;
            padding: 20px;
        }
        h1 {
            text-align: center;
            margin-bottom: 20px;
        }
        .links {
            text-align: center;
            margin-bottom: 20px;
        }
        .links a {
            margin: 0 10px;
            text-decoration: none;
            color: #007BFF;
        }
        .section {
            margin-bottom: 40px;
        }
        .section img {
            width: 100%;
            height: auto;
            margin-bottom: 20px;
        }
        .framework-images {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        .framework-images img {
            width: 100%;
            height: auto;
        }
        hr {
            border: 0;
            height: 1px;
            background: #333;
            background-image: linear-gradient(to right, #ccc, #333, #ccc);
            margin-bottom: 40px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        table, th, td {
            border: 1px solid black;
        }
        th, td {
            padding: 8px;
            text-align: center;
        }
        th {
            background-color: #f2f2f2;
            cursor: pointer;
        }
        .buttons {
            text-align: center;
            margin-bottom: 20px;
        }
        .buttons button {
            margin: 0 10px;
            padding: 10px 20px;
            cursor: pointer;
        }
        .dataTables_length, .dataTables_filter, .dataTables_info, .dataTables_paginate {
            display: none;
        }
    </style>
    <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.21/css/jquery.dataTables.css">
</head>
<body>
    <div class="container">
        <h1>KnowBLIP: Large Vision-Language Models meet Interleaved World Knowledge</h1>
        <div class="links">
            <a href="https://drive.google.com/drive/folders/1ZgfApnIhzXg8n3R_kwikix5C0Klv6UP8?usp=drive_link">Dataset</a>
        </div>
        <div class="section">
            <h2>Abstract</h2>
            <p>Currently, Large Visual Language Models (LVLMs) have integrated various tasks, and some ``any2any'' models have emerged, enabling support for diverse modalities for input and various output formats, thereby expanding the impact of LVLMs. However, we note that existing models remain incomplete and distant from real-world applicability, primarily due to two key factors: partial functionality and lack of connection with world knowledge. Firstly, we stress the importance of a comprehensive LVLM that accommodates various input and output paradigms, which should be interleaved, multi-turn, and multi-lingual. However, there is no multi-modal framework capable of fully encompassing all functionalities. Moreover, textual knowledge often lacks substantial connections with the visible objective world, resulting in visual and language alignment limited to the semantic level, detached from objective facts. This limitation leads to models generated primarily through imagination rather than a grounded understanding of the world. To bridge the gap between LVLMs and the visible real world, we propose a novel multi-modal framework, KnowBLIP, designed to support a comprehensive, world-grounded, ``any2any'' paradigm. Specifically, we have collected a dataset, IWK-500k, of multi-modal objective knowledge derived from reasoning on multi-modal knowledge graphs, characterized by interleaved, multi-turn, and multi-lingual attributes. Furthermore, we have developed the KnowBLIP, capable of accommodating various input and output paradigms based on IWK-500k dataset. Extensive experiments have demonstrated the effectiveness of our approach.</p>
        </div>
        <hr>
        <div class="section">
            <h2>Framework</h2>
            <p>To bridge the gap between LVLMs and the visible real world, we propose a novel multi-modal framework, KnowBLIP, designed to support a comprehensive, world-grounded, ``any2any'' paradigm.</p>
            <img src="./framework.png" alt="Dataset images">
        </div>
        <hr>
    <div class="section">
        <h2>Leaderboard</h2>
        <p>We evaluate various models including LLMs and LMMs. In each type, we consider both closed- and open-source models. Our evaluation is conducted under a zero-shot setting to assess the capability of models to generate accurate answers without fine-tuning or few-shot demonstrations on our benchmark. For all models, we use the default prompt provided by each model for multi-choice or open QA, if available. If models do not provide prompts for task types in MMU, we conduct prompt engineering on the validation set and use the most effective prompt for the later zero-shot experiment.</p>
        <div class="buttons">
            <button onclick="showTable('zeroShot')">Zero-Shot</button>
            <button onclick="showTable('Trained')">Trained</button>
        </div>
        <table id="zeroShotTable" class="display">
            <thead>
                <tr>
                    <th>Model</th>
                    <th>Text2Text Acc</th>
                    <th>Image2Text Acc</th>
                    <th>Text2Image R@1</th>
                    <th>Text2Image R@5</th>
                    <th>Image2Image R@1</th>
                    <th>Image2Image R@5</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>VisualGLM</td>
                    <td>8.54%</td>
                    <td>7.53%</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>Qwen-VL</td>
                    <td>14.57%</td>
                    <td>12.56%</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>MiniGPTV2</td>
                    <td>12.06%</td>
                    <td>1.50%</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>GILL</td>
                    <td>8.10%</td>
                    <td>11.75%</td>
                    <td>0.00%</td>
                    <td>2.95%</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>TextBind</td>
                    <td>18.20%</td>
                    <td>21.02%</td>
                    <td>2.98%</td>
                    <td>4.13%</td>
                    <td>2.21%</td>
                    <td>4.70%</td>
                </tr>
                <tr>
                    <td>LLaVA-1.5-7B</td>
                    <td>6.09%</td>
                    <td>10.20%</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>CogVLM-chat-v1.1</td>
                    <td>14.45%</td>
                    <td>17.41%</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>GPT-4V</td>
                    <td>61.53%</td>
                    <td>55.88%</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>Gemini-1.5-pro</td>
                    <td>56.41%</td>
                    <td>41.53%</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
            </tbody>
        </table>
        <table id="trainedTable" class="display" style="display:none">
            <thead>
                <tr>
                    <th>Model</th>
                    <th>Text2Text Acc</th>
                    <th>Image2Text Acc</th>
                    <th>Text2Image R@1</th>
                    <th>Text2Image R@5</th>
                    <th>Image2Image R@1</th>
                    <th>Image2Image R@5</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>VisualGLM</td>
                    <td>50.72%</td>
                    <td>22.50%</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>Qwen-VL</td>
                    <td>51.69%</td>
                    <td>48.74%</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>MiniGPTV2</td>
                    <td>40.62%</td>
                    <td>30.57%</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>GILL</td>
                    <td>47.38%</td>
                    <td>70.03%</td>
                    <td>6.79%</td>
                    <td>24.30%</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>TextBind</td>
                    <td>59.38%</td>
                    <td>64.29%</td>
                    <td>5.66%</td>
                    <td>6.30%</td>
                    <td>3.72%</td>
                    <td>6.79%</td>
                </tr>
                <tr>
                    <td>KnowBLIP (Ours) </td>
                    <td>66.73%</td>
                    <td>70.60%</td>
                    <td>29.71%</td>
                    <td>78.24%</td>
                    <td>17.47</td>
                    <td>64.38</td>
                </tr>
            </tbody>
        </table>
    </div>
        <hr>
        <div class="section">
            <h2>Case Study</h2>
            <p>The four formats of the IWK-300K dataset endow KnowBLIP with a diverse array of functionalities, enabling it to handle a wide range of tasks and scenarios related to real world knowledge. To illustrate these capabilities, we present several concrete user cases.</p>
            <div class="framework-images">
                <img src="./kuancase11.jpg" alt="Framework image 1">
                <img src="./kuancase22.jpg" alt="Framework image 2">
                <img src="./kuancase33.jpg" alt="Framework image 3">
                <img src="./kuancase44.jpg" alt="Framework image 1">
                <img src="./kuancase55.jpg" alt="Framework image 2">
                <img src="./kuancase66.jpg" alt="Framework image 3">
            </div>
        </div>
    </div>
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <script src="https://cdn.datatables.net/1.10.21/js/jquery.dataTables.min.js"></script>
    <script>
        $(document).ready(function() {
            $('#zeroShotTable').DataTable();
            $('#trainedTable').DataTable();
        });

        function showTable(tableId) {
            if (tableId === 'zeroShot') {
                $('#zeroShotTable').show();
                $('#trainedTable').hide();
            } else if (tableId === 'trained') {
                $('#zeroShotTable').hide();
                $('#trainedTable').show();
            }
        }
    </script>
</body>
</html>
